{"id": 284219537, "abstract": "In this paper we propose a new method for regularizing autoencoders by\nimposing an arbitrary prior on the latent representation of the autoencoder.\nOur method, named \"adversarial autoencoder\", uses the recently proposed\ngenerative adversarial networks (GAN) in order to match the aggregated\nposterior of the hidden code vector of the autoencoder with an arbitrary prior.\nMatching the aggregated posterior to the prior ensures that there are no\n\"holes\" in the prior, and generating from any part of prior space results in\nmeaningful samples. As a result, the decoder of the adversarial autoencoder\nlearns a deep generative model that maps the imposed prior to the data\ndistribution. We show how adversarial autoencoders can be used to disentangle\nstyle and content of images and achieve competitive generative performance on\nMNIST, Street View House Numbers and Toronto Face datasets.", "cluster": "3", "citations": [], "references": [41107019, 281487457, 269935510, 263012109, 7017915, 259400035, 263316327, 224310755, 266031774, 262991675], "authors": ["Alireza Makhzani", "Jonathon Shlens", "Navdeep Jaitly", "Ian Goodfellow"], "title": "Adversarial Autoencoders"}