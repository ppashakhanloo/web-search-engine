{"id": 234108809, "abstract": "We propose a new class of learning algorithms that combines variational\napproximation and Markov chain Monte Carlo (MCMC) simulation. Naive algorithms\nthat use the variational approximation as proposal distribution can perform\npoorly because this approximation tends to underestimate the true variance and\nother features of the data. We solve this problem by introducing more\nsophisticated MCMC algorithms. One of these algorithms is a mixture of two MCMC\nkernels: a random walk Metropolis kernel and a blockMetropolis-Hastings (MH)\nkernel with a variational approximation as proposaldistribution. The MH kernel\nallows one to locate regions of high probability efficiently. The Metropolis\nkernel allows us to explore the vicinity of these regions. This algorithm\noutperforms variationalapproximations because it yields slightly better\nestimates of the mean and considerably better estimates of higher moments, such\nas covariances. It also outperforms standard MCMC algorithms because it locates\ntheregions of high probability quickly, thus speeding up convergence. We\ndemonstrate this algorithm on the problem of Bayesian parameter estimation for\nlogistic (sigmoid) belief networks.", "cluster": "5", "citations": [4221147, 2574717, 227276601, 221432360, 238689222, 2881520, 221618137, 3414178, 224320171, 49458431], "references": [2750746, 33020867, 2281698, 2783885, 4895745, 243766743, 45622656, 265465075, 200104426, 237104492], "authors": ["Nando de Freitas", "Pedro Hojen-Sorensen", "Michael I. Jordan", "Stuart Russell"], "title": "Variational MCMC"}