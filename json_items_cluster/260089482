{"id": 260089482, "abstract": "The recently developed Bayesian Gaussian process latent variable model\n(GPLVM) is a powerful generative model for discovering low dimensional\nembeddings in linear time complexity. However, modern datasets are so large\nthat even linear-time models find them difficult to cope with. We introduce a\nnovel re-parametrisation of variational inference for the GPLVM and sparse GP\nmodel that allows for an efficient distributed inference algorithm.\nWe present a unifying derivation for both models, analytically deriving the\noptimal variational distribution over the inducing points. We then assess the\nsuggested inference on datasets of different sizes, showing that it scales well\nwith both data and computational resources. We furthermore demonstrate its\npracticality in real-world settings using datasets with up to 100 thousand\npoints, comparing the inference to sequential implementations, assessing the\ndistribution of the load among the different nodes, and testing its robustness\nto network failures.", "cluster": "2", "citations": [281486855, 283296484, 278047923, 269417452, 269116839, 267157313, 262840808, 268079368, 273388187, 277959103], "references": [220320635, 220320048, 257069490, 220851866, 228092206, 220320714, 41781406, 222482794, 221619032, 261683938], "authors": ["Yarin Gal", "Mark van der Wilk", "Carl E. Rasmussen"], "title": "Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models"}