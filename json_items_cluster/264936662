{"id": 264936662, "abstract": "We consider the problem of maximizing a real-valued continuous function $f$\nusing a Bayesian approach. Since the early work of Jonas Mockus and Antanas\n\\v{Z}ilinskas in the 70's, the problem of optimization is usually formulated by\nconsidering the loss function $\\max f - M_n$ (where $M_n$ denotes the best\nfunction value observed after $n$ evaluations of $f$). This loss function puts\nemphasis on the value of the maximum, at the expense of the location of the\nmaximizer. In the special case of a one-step Bayes-optimal strategy, it leads\nto the classical Expected Improvement (EI) sampling criterion. This is a\nspecial case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk\nassociated to a certain uncertainty measure (here, the expected loss) on the\nquantity of interest is minimized at each step of the algorithm. In this\narticle, assuming that $f$ is defined over a measure space $(\\mathbb{X},\n\\lambda)$, we propose to consider instead the integral loss function\n$\\int_{\\mathbb{X}} (f - M_n)_{+}\\, d\\lambda$, and we show that this leads, in\nthe case of a Gaussian process prior, to a new numerically tractable sampling\ncriterion that we call $\\rm EI^2$ (for Expected Integrated Expected\nImprovement). A numerical experiment illustrates that a SUR strategy based on\nthis new sampling criterion reduces the error on both the value and the\nlocation of the maximizer faster than the EI-based strategy.", "cluster": "4", "citations": [282570552], "references": [46587486, 228438518, 258831560, 235709802, 242922491, 248818761, 257299288, 1960302, 264936662, 264936662], "authors": ["Emmanuel Vazquez", "Julien Bect"], "title": "A new integral loss function for Bayesian optimization"}