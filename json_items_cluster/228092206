{"id": 228092206, "abstract": "We develop stochastic variational inference, a scalable algorithm for\napproximating posterior distributions. We develop this technique for a large\nclass of probabilistic models and we demonstrate it with two probabilistic\ntopic models, latent Dirichlet allocation and the hierarchical Dirichlet\nprocess topic model. Using stochastic variational inference, we analyze several\nlarge collections of documents: 300K articles from Nature, 1.8M articles from\nThe New York Times, and 3.8M articles from Wikipedia. Stochastic inference can\neasily handle data sets of this size and outperforms traditional variational\ninference, which can only handle a smaller subset. (We also show that the\nBayesian nonparametric topic model outperforms its parametric counterpart.)\nStochastic variational inference lets us apply complex Bayesian models to\nmassive data sets.", "cluster": "0", "citations": [287209198, 286513408, 284579582, 284476621, 283762222, 283433373, 283334510, 283117540, 283043558, 281262237], "references": [38358824, 221620102, 226435002, 5948829, 221620547, 221345245, 45865402, 221520030, 220698851, 2850032], "authors": ["Matt Hoffman", "David M. Blei", "Chong Wang", "John Paisley"], "title": "Stochastic Variational Inference"}