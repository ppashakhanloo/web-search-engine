{"id": 267983152, "abstract": "Probabilistic programming languages can simplify the development of machine\nlearning techniques, but only if inference is sufficiently scalable.\nUnfortunately, Bayesian parameter estimation for highly coupled models such as\nregressions and state-space models still scales badly. This paper describes a\nsublinear-time algorithm for making Metropolis-Hastings updates to latent\nvariables in probabilistic programs. This approach generalizes recently\nintroduced approximate MH techniques: instead of subsampling data items assumed\nto be independent, it subsamples edges in a dynamically constructed graphical\nmodel. It thus applies to a broader class of problems and interoperates with\ngeneral-purpose inference techniques. Empirical results are presented for\nBayesian logistic regression, nonlinear classification via joint Dirichlet\nprocess mixtures, and parameter estimation for stochastic volatility models\n(with state estimation via particle MCMC). All three applications use the same\nimplementation, and each requires under 20 lines of probabilistic code.", "cluster": "4", "citations": [], "references": [262157385, 261508268, 261289213, 236235190, 262350854, 229100346], "authors": ["Yutian Chen", "Vikash Mansinghka", "Zoubin Ghahramani"], "title": "Sublinear Approximate Inference for Probabilistic Programs"}