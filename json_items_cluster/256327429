{"id": 256327429, "abstract": "We present an LDA approach to entity disambiguation. Each topic is associated\nwith a Wikipedia article and topics generate either content words or entity\nmentions. Training such models is challenging because of the topic and\nvocabulary size, both in the millions. We tackle these problems using a novel\ndistributed inference and representation framework based on a parallel Gibbs\nsampler guided by the Wikipedia link graph, and pipelines of MapReduce allowing\nfast and memory-frugal processing of large datasets. We report state-of-the-art\nperformance on a public dataset.", "cluster": "0", "citations": [], "references": [224513847, 221620547, 220752324, 220439168, 3297670, 220851866, 5329578, 220269845, 228790255, 221618332], "authors": ["Neil Houlsby", "Massimiliano Ciaramita"], "title": "Scalable Probabilistic Entity-Topic Modeling"}