{"id": 4742228, "abstract": "Many of the classification algorithms developed in the machine learning literature, including the support vector machine and boosting, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. The convexity makes these algorithms computationally e#cient. The use of a surrogate, however, has statistical consequences that must be balanced against the computational virtues of convexity. To study these issues, we provide a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function: that it satisfy a pointwise form of Fisher consistency for classification. The relationship is based on a simple variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise.", "cluster": "3", "citations": [282639101, 280941090, 281947189, 277959055, 277959353, 274967535, 273067270, 272194961, 270287460, 269930822], "references": [2560520, 225670544, 3080728, 33020867, 38351334, 260303138, 4742653, 200033836, 221344849, 2376111], "authors": ["Peter L. Bartlett", "Michael I. Jordan", "Jon D. McAuliffe"], "title": "Convexity, Classification, and Risk Bounds"}