{"id": 7017915, "abstract": "We show how to use \"complementary priors\" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.", "cluster": "3", "citations": [283730103, 289928157, 288872405, 285392625, 281450780, 287250079, 286919855, 281073786, 287994861, 284913577], "references": [10655048, 15437023, 11207765, 220860992, 3193348, 221364240, 2985446, 3328006, 2729058, 222464584], "authors": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "title": "A Fast Learning Algorithm for Deep Belief Nets"}