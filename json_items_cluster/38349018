{"id": 38349018, "abstract": "Classification can be considered as nonparametric estimation of sets, where the risk is defined by means of a specific distance between sets associated with misclassification error. It is shown that the rates of convergence of classifiers depend on two parameters: the complexity of the class of candidate sets and the margin parameter. The dependence is explicitly given, indicating that optimal fast rates approaching $O(n^{-1})$ can be attained, where n is the sample size, and that the proposed classifiers have the property of robustness to the margin. The main result of the paper concerns optimal aggregation of classifiers: we suggest a classifier that automatically adapts both to the complexity and to the margin, and attains the optimal fast rates, up to a logarithmic factor.", "cluster": "0", "citations": [289530299, 279968247, 270906191, 270594292, 252053136, 264862660, 263163848, 262640919, 259915130, 259804031], "references": [226270700, 44241055, 38361518, 23695103, 230675276, 220568035, 38348323, 226469004, 226869786, 45622657], "authors": ["Alexander B. Tsybakov"], "title": "Optimal aggregation of classifiers in statistical learning, Annals of Statististics, 32(1)"}