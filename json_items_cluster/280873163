{"id": 280873163, "abstract": "Many real-world problems involve massive amounts of data. Under these\ncircumstances learning algorithms often become prohibitively expensive, making\nscalability a pressing issue to be addressed. A common approach is to perform\nsampling to reduce the size of the dataset and enable efficient learning.\nAlternatively, one customizes learning algorithms to achieve scalability. In\neither case, the key challenge is to obtain algorithmic efficiency without\ncompromising the quality of the results. In this paper we discuss a\nmeta-learning algorithm (PSBML) which combines features of parallel algorithms\nwith concepts from ensemble and boosting methodologies to achieve the desired\nscalability property. We present both theoretical and empirical analyses which\nshow that PSBML preserves a critical property of boosting, specifically,\nconvergence to a distribution centered around the margin. We then present\nadditional empirical analyses showing that this meta-level algorithm provides a\ngeneral and effective framework that can be used in combination with a variety\nof learning classifiers. We perform extensive experiments to investigate the\ntradeoff achieved between scalability and accuracy, and robustness to noise, on\nboth synthetic and real-world data. These empirical results corroborate our\ntheoretical analysis, and demonstrate the potential of PSBML in achieving\nscalability without sacrificing accuracy.", "cluster": "0", "citations": [], "references": [220321015, 235677367, 262348479, 2763972, 3418844, 2570628, 2458795, 244498683, 2455627, 226612208], "authors": ["Uday Kamath", "Carlotta Domeniconi", "Kenneth De Jong"], "title": "Theoretical and Empirical Analysis of a Parallel Boosting Algorithm"}