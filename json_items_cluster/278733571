{"id": 278733571, "abstract": "Relatively high computational cost for Bayesian methods often limits their\napplication for big data analysis. In recent years, there have been many\nattempts to improve computational efficiency of Bayesian inference. Here we\npropose an efficient and scalable computational technique for a\nstate-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian\nMonte Carlo (HMC). The key idea is to explore and exploit the regularity in\nparameter space for the underlying probabilistic model to construct an\neffective approximation of the collective geometric and statistical properties\nof the whole observed data. To this end, we use shallow neural networks along\nwith efficient learning algorithms. The choice of basis functions (or hidden\nunits in neural networks) and the optimized learning process provides a\nflexible, scalable and efficient sampling algorithm. Experiments based on\nsimulated and real data show that our approach leads to substantially more\nefficient sampling algorithms compared to existing state-of-the art methods.", "cluster": "5", "citations": [277959058], "references": [200105001, 259695171, 227375782, 260107754, 3303476, 221346425, 225291078, 228926940, 200175478, 222667300], "authors": ["Cheng Zhang", "Babak Shahbaba", "Hongkai Zhao"], "title": "Hamiltonian Monte Carlo Acceleration Using Neural Network Surrogate functions"}