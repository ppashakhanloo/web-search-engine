{"id": 273157486, "abstract": "Despite having various attractive qualities such as high prediction accuracy\nand the ability to quantify uncertainty and avoid over-fitting, Bayesian Matrix\nFactorization has not been widely adopted because of the prohibitive cost of\ninference. In this paper, we propose a scalable distributed Bayesian matrix\nfactorization algorithm using stochastic gradient MCMC. Our algorithm, based on\nDistributed Stochastic Gradient Langevin Dynamics, can not only match the\nprediction accuracy of standard MCMC methods like Gibbs sampling, but at the\nsame time is as fast and simple as stochastic gradient descent. In our\nexperiments, we show that our algorithm can achieve the same level of\nprediction accuracy as Gibbs sampling an order of magnitude faster. We also\nshow that our method reduces the prediction error as fast as distributed\nstochastic gradient descent, achieving a 4.1% improvement in RMSE for the\nNetflix dataset and an 1.8% for the Yahoo music dataset.", "cluster": "5", "citations": [291185091, 278047801, 275974197], "references": [45908331, 228095582, 260232379, 269692923, 241060215, 221653420, 227375782, 228790255, 236235190, 224574578], "authors": ["Sungjin Ahn", "Anoop Korattikara", "Nathan Liu", "Suju Rajan", "Max Welling"], "title": "Large-Scale Distributed Bayesian Matrix Factorization using Stochastic Gradient MCMC"}