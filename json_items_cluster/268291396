{"id": 268291396, "abstract": "We consider the problem of selecting, from among the arms of a stochastic n-armed bandit, a subset of size m of those arms with the highest expected rewards, based on efficiently sampling the arms. This \"sub-set selection\" problem finds application in a variety of areas. In the authors' previ-ous work (Kalyanakrishnan & Stone, 2010), this problem is framed under a PAC setting (denoted \"Explore-m\"), and corresponding sampling algorithms are analyzed. Whereas the formal analysis therein is restricted to the worst case sample complexity of algo-rithms, in this paper, we design and ana-lyze an algorithm (\"LUCB\") with improved expected sample complexity. Interestingly LUCB bears a close resemblance to the well-known UCB algorithm for regret minimiza-tion. The expected sample complexity bound we show for LUCB is novel even for single-arm selection (Explore-1). We also give a lower bound on the worst case sample com-plexity of PAC algorithms for Explore-m.", "cluster": "4", "citations": [277924329, 263545276, 234061312, 224959372, 262487539, 269270916, 259478458, 277924144, 269307620, 261836774], "references": [220320322, 221497549, 2942622, 2788600, 221344896, 220343796, 221346082, 268291396, 268291396, 268291396], "authors": ["Shivaram Kalyanakrishnan", "Ambuj Tewari", "Peter Auer", "Peter Stone"], "title": "PAC Subset Selection in Stochastic Multi-armed Bandits"}