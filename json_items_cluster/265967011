{"id": 265967011, "abstract": "Many signal processing and machine learning methods share essentially the\nsame linear-in-the-parameter model, with as many parameters as available\nsamples as in kernel-based machines. Sparse approximation is essential in many\ndisciplines, with new challenges emerging in online learning with kernels. To\nthis end, several sparsity measures have been proposed in the literature to\nquantify sparse dictionaries and constructing relevant ones, the most prolific\nones being the distance, the approximation, the coherence and the Babel\nmeasures. In this paper, we analyze sparse dictionaries based on these\nmeasures. By conducting an eigenvalue analysis, we show that these sparsity\nmeasures share many properties, including the linear independence condition and\ninducing a well-posed optimization problem. Furthermore, we prove that there\nexists a quasi-isometry between the parameter (i.e., dual) space and the\ndictionary's induced feature space.", "cluster": "2", "citations": [265967012, 267759699, 281127386], "references": [2237665, 220279290, 41781139, 3318965, 3085174, 220691600, 224355964, 261080300, 260114578, 220279398], "authors": ["Paul Honeine"], "title": "Analyzing Sparse Dictionaries for Online Learning With Kernels"}