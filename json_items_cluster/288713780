{"id": 288713780, "abstract": "Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian\nanalogs to popular stochastic optimization methods; however, this connection is\nnot well studied. We explore this relationship by applying simulated annealing\nto an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two\nkey components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii)\nadaptive element-wise momentum weights. The zero-temperature limit gives a\nnovel stochastic optimization method with adaptive element-wise momentum\nweights, while conventional optimization methods only have a shared, static\nmomentum weight. Under certain assumptions, our theoretical analysis suggests\nthe proposed simulated annealing approach converges close to the global optima.\nExperiments on several deep neural network models show state-of-the-art results\ncompared to related stochastic optimization algorithms.", "cluster": "5", "citations": [], "references": [234140081, 228671354, 228095594, 260232379, 262877889, 272423025, 269692923, 220320677, 278244488, 239582331], "authors": ["Changyou Chen", "David Carlson", "Zhe Gan", "Chunyuan Li", "Lawrence Carin"], "title": "Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization"}