{"id": 284219020, "abstract": "The standard unsupervised recurrent neural network language model (RNNLM)\ngenerates sentences one word at a time and does not work from an explicit\nglobal distributed sentence representation. In this work, we present an\nRNN-based variational autoencoder language model that incorporates distributed\nlatent representations of entire sentences. This factorization allows it to\nexplicitly model holistic properties of sentences such as style, topic, and\nhigh-level syntactic features. Samples from the prior over these sentence\nrepresentations remarkably produce diverse and well-formed sentences through\nsimple deterministic decoding. By examining paths through this latent space, we\nare able to generate coherent novel sentences that interpolate between known\nsentences. We present techniques for solving the difficult learning problem\npresented by this model, demonstrate strong performance in the imputation of\nmissing tokens, and explore many interesting properties of the latent sentence\nspace.", "cluster": "2", "citations": [], "references": [265252627, 268988199, 233950933, 277959228, 283531914, 278733352, 268525836, 269997903, 263012109, 272422583], "authors": ["Samuel R. Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M. Dai", "Rafal Jozefowicz", "Samy Bengio"], "title": "Generating Sentences from a Continuous Space"}