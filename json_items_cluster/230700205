{"id": 230700205, "abstract": "Many different machine learning algorithms exist; taking into account each\nalgorithm's hyperparameters, there is a staggeringly large number of possible\nalternatives overall. We consider the problem of simultaneously selecting a\nlearning algorithm and setting its hyperparameters, going beyond previous work\nthat addresses these issues in isolation. We show that this problem can be\naddressed by a fully automated approach, leveraging recent innovations in\nBayesian optimization. Specifically, we consider a wide range of feature\nselection techniques (combining 3 search and 8 evaluator methods) and all\nclassification approaches implemented in WEKA, spanning 2 ensemble methods, 10\nmeta-methods, 27 base classifiers, and hyperparameter settings for each\nclassifier. On each of 21 popular datasets from the UCI repository, the KDD Cup\n09, variants of the MNIST dataset and CIFAR-10, we show classification\nperformance often much better than using standard selection/hyperparameter\noptimization methods. We hope that our approach will help non-expert users to\nmore effectively identify machine learning algorithms and hyperparameter\nsettings appropriate to their applications, and hence to achieve improved\nperformance.", "cluster": "3", "citations": [286792242, 283659038, 275966269, 275966194, 277958776, 266622312, 279274976, 263773784, 260725509, 261124553], "references": [221900777, 225237917, 216816964, 2360567, 221608790, 220344215, 260345678, 221345088, 262395872, 2375370], "authors": ["Chris Thornton", "Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "title": "Auto-WEKA: Combined Selection and Hyperparameter Optimization of\nClassification Algorithms"}