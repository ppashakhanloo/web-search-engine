{"id": 271855369, "abstract": "The past decades have seen enormous improvements in computational inference\nbased on statistical models, with continual enhancement in a wide range of\ncomputational tools, in competition. In Bayesian inference, first and foremost,\nMCMC techniques continue to evolve, moving from random walk proposals to\nLangevin drift, to Hamiltonian Monte Carlo, and so on, with both theoretical\nand algorithmic inputs opening wider access to practitioners. However, this\nimpressive evolution in capacity is confronted by an even steeper increase in\nthe complexity of the models and datasets to be addressed. The difficulties of\nmodelling and then handling ever more complex datasets most likely call for a\nnew type of tool for computational inference that dramatically reduce the\ndimension and size of the raw data while capturing its essential aspects.\nApproximate models and algorithms may thus be at the core of the next\ncomputational revolution.", "cluster": "5", "citations": [280329849, 275363896], "references": [228924013, 220286454, 228636543, 263679121, 234145912, 1771219, 265347313, 4772293, 45897328, 45866217], "authors": ["Peter J. Green", "Krzysztof Łatuszyński", "Marcelo Pereyra", "Christian P. Robert"], "title": "Bayesian computation: a perspective on the current state, and sampling backwards and forwards"}