{"id": 2495015, "abstract": "Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set.", "cluster": "2", "citations": [253330477, 266261674, 262641962, 262532661, 258373922, 229157546, 224176534, 220320926, 224149764, 229350044], "references": [2280218, 33020867, 2281698, 2239998, 243712636, 216301244, 221619568, 2239690, 2251448, 224881830], "authors": ["Zoubin Ghahramani", "Matthew J. Beal"], "title": "Propagation Algorithms for Variational Bayesian Learning"}