{"id": 2239998, "abstract": "This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally nonGaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation. 1 Introduction A standard method to learn a graphical model 1 from data is maximum likelihood (ML). Given a training dataset, ML estimates a single optimal value f...", "cluster": "3", "citations": [275897326, 273396836, 265787924, 262641962, 263710403, 263898885, 262532661, 261567288, 261598118, 259625316], "references": [2280218, 13072654, 2457314, 2348531, 221619568], "authors": ["Hagai Attias"], "title": "A Variational Bayesian Framework for Graphical Models"}