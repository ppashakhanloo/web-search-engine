{"id": 281058916, "abstract": "The Markov Chain Monte Carlo method is the dominant paradigm for posterior\ncomputation in Bayesian analysis. It has long been common to control\ncomputation time by making approximations to the Markov transition kernel.\nComparatively little attention has been paid to convergence and estimation\nerror in these approximating Markov Chains. We propose a framework for\nassessing when to use approximations in MCMC algorithms, and how much error in\nthe transition kernel should be tolerated to obtain optimal estimation\nperformance with respect to a specified loss function and computational budget.\nThe results require only ergodicity of the exact kernel and control of the\nkernel approximation accuracy. The theoretical framework is applied to\napproximations based on random subsets of data, low-rank approximations of\nGaussian processes, and a novel approximating Markov chain for discrete mixture\nmodels.", "cluster": "5", "citations": [281262237, 291353622], "references": [261065488, 244416205, 224881739, 243774781, 236255250, 40694044, 220116484, 24268554, 236235190, 224904690], "authors": ["James E. Johndrow", "Jonathan C. Mattingly", "Sayan Mukherjee", "David Dunson"], "title": "Approximations of Markov Chains and High-Dimensional Bayesian Inference"}