{"id": 230732835, "abstract": "In stochastic variational inference, the variational Bayes objective function\nis optimized using stochastic gradient approximation, where gradients computed\non small random subsets of data are used to approximate the true gradient over\nthe whole data set. This enables complex models to be fit to large data sets as\ndata can be processed in mini-batches. In this article, we extend stochastic\nvariational inference for conjugate-exponential models to nonconjugate models\nand present a stochastic nonconjugate variational message passing algorithm for\nfitting generalized linear mixed models that is scalable to large data sets. In\naddition, we show that diagnostics for prior-likelihood conflict, which are\nuseful for Bayesian model criticism, can be obtained from nonconjugate\nvariational message passing automatically, as an alternative to\nsimulation-based Markov chain Monte Carlo methods. Finally, we demonstrate that\nfor moderate-sized data sets, convergence can be accelerated by using the\nstochastic version of nonconjugate variational message passing in the initial\nstage of optimization before switching to the standard version.", "cluster": "4", "citations": [267214327, 237082504, 262569028, 272845343, 289587906], "references": [254212821, 228095632, 228914332, 51887545, 20490246, 15218628, 2239998, 220320763, 261622599, 1762088], "authors": ["Linda S. L. Tan", "David J. Nott"], "title": "A Stochastic Variational Framework for Fitting and Diagnosing Generalized Linear Mixed Models"}