{"id": 277959103, "abstract": "We show that a multilayer perceptron (MLP) with arbitrary depth and\nnonlinearities, with dropout applied after every weight layer, is\nmathematically equivalent to an approximation to a well known Bayesian model.\nThis interpretation offers an explanation to some of dropout's key properties,\nsuch as its robustness to over-fitting. Our interpretation allows us to reason\nabout uncertainty in deep learning, and allows the introduction of the Bayesian\nmachinery into existing deep learning frameworks in a principled way.\nThis document is an appendix for the main paper \"Dropout as a Bayesian\nApproximation: Representing Model Uncertainty in Deep Learning\" by Gal and\nGhahramani, 2015.", "cluster": "3", "citations": [], "references": [277022910, 232805135, 273388187, 260089482, 267706055, 257069490, 13853244, 228092206, 259400035, 267960550], "authors": ["Yarin Gal", "Zoubin Ghahramani"], "title": "Dropout as a Bayesian Approximation: Appendix"}