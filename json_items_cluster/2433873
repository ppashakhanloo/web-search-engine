{"id": 2433873, "abstract": "When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural graadient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behaviour of natural gradient online learning is analyzed and is proved to be Fischer efficient, implying that it has assymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.", "cluster": "5", "citations": [289406962, 284579051, 284476101, 283531387, 283532009, 283043539, 281768642, 281768579, 280869374, 280104564], "references": [243780082, 245588333, 221165555, 243775838, 3377007, 242919207, 13383386, 242395812, 239665838, 2815942], "authors": ["Shun-ichi Amari"], "title": "Natural Gradient Works Efficiently in Learning"}