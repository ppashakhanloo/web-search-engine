{"id": 284219954, "abstract": "Deep neural networks are powerful parametric models that can be trained\nefficiently using the backpropagation algorithm. Stochastic neural networks\ncombine the power of large parametric functions with that of graphical models,\nwhich makes it possible to learn very complex distributions. However, as\nbackpropagation is not directly applicable to stochastic networks that include\ndiscrete sampling operations within their computational graph, training such\nnetworks remains difficult. We present MuProp, an unbiased gradient estimator\nfor stochastic networks, designed to make this task easier. MuProp improves on\nthe likelihood-ratio estimator by reducing its variance using a control variate\nbased on the first-order Taylor expansion of a mean-field network. Crucially,\nunlike prior attempts at using backpropagation for training stochastic\nnetworks, the resulting estimator is unbiased and well behaved. Our experiments\non structured output prediction and discrete latent variable modeling\ndemonstrate that MuProp yields consistently good performance across a range of\ndifficult tasks.", "cluster": "3", "citations": [], "references": [255909499, 15614028, 260637318, 15437023, 259400035, 280557504, 260022128, 263390366, 224685146, 229091480], "authors": ["Shixiang Gu", "Sergey Levine", "Ilya Sutskever", "Andriy Mnih"], "title": "MuProp: Unbiased Backpropagation for Stochastic Neural Networks"}