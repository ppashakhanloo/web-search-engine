{"id": 287249271, "abstract": "Gaussian Processes (GPs) are widely used tools in statistics, machine\nlearning, robotics, computer vision, and scientific computation. However,\ndespite their popularity, they can be difficult to apply; all but the simplest\nclassification or regression applications require specification and inference\nover complex covariance functions that do not admit simple analytical\nposteriors. This paper shows how to embed Gaussian processes in any\nhigher-order probabilistic programming language, using an idiom based on\nmemoization, and demonstrates its utility by implementing and extending classic\nand state-of-the-art GP applications. The interface to Gaussian processes,\ncalled gpmem, takes an arbitrary real-valued computational process as input and\nreturns a statistical emulator that automatically improve as the original\nprocess is invoked and its input-output behavior is recorded. The flexibility\nof gpmem is illustrated via three applications: (i) robust GP regression with\nhierarchical hyper-parameter learning, (ii) discovering symbolic expressions\nfrom time-series data by fully Bayesian structure learning over kernels\ngenerated by a stochastic grammar, and (iii) a bandit formulation of Bayesian\noptimization with automatic inference and action selection. All applications\nshare a single 50-line Python library and require fewer than 20 lines of\nprobabilistic code each.", "cluster": "2", "citations": [], "references": [270311452, 285697817, 284868064, 232805135, 221344546, 261065571, 260250756, 241683009, 256655975, 1735582], "authors": ["Ulrich Schaechtle", "Ben Zinberg", "Alexey Radul", "Kostas Stathis", "Vikash K. Mansinghka"], "title": "Probabilistic Programming with Gaussian Process Memoization"}