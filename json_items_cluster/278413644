{"id": 278413644, "abstract": "We present doubly stochastic gradient MCMC, a simple and generic method for\n(approximate) Bayesian inference of deep generative models in the collapsed\ncontinuous parameter space. At each MCMC sampling step, the algorithm randomly\ndraws a mini-batch of data samples to estimate the gradient of log-posterior\nand further estimates the intractable expectation over latent variables via a\nGibbs sampler or a neural adaptive importance sampler. We demonstrate the\neffectiveness on learning deep sigmoid belief networks (DSBNs). Compared to the\nstate-of-the-art methods using Gibbs sampling with data augmentation, our\nalgorithm is much more efficient and manages to learn DSBNs on large datasets.", "cluster": "5", "citations": [], "references": [237053970, 7017915, 200744453, 278244188, 45893028, 226435002, 1961033, 268209561, 262991675, 221346425], "authors": ["Chao Du", "Jun Zhu", "Bo Zhang"], "title": "Learning Deep Generative Models with Doubly Stochastic MCMC"}