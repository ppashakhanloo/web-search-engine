{"id": 281487831, "abstract": "While deep networks show to be highly effective in extensive applications,\nfew efforts have been spent on studying its potential in clustering. In this\npaper, we argue that the successful domain expertise of sparse coding in\nclustering is still valuable, and can be combined with the key ingredients of\ndeep learning. A novel feed-forward architecture, named TAG-LISTA, is\nconstructed from graph-regularized sparse coding. It is then trained with\ntask-specific loss functions from end to end. The inner connections of the\nproposed network to sparse coding leads to more effective training. Moreover,\nby introducing auxiliary clustering tasks to the hierarchy of intermediate\nfeatures, we present DTAG-LISTA and obtain a further performance boost. We\ndemonstrate extensive experiments on several benchmark datasets, under a wide\nvariety of settings. The results verify that the proposed model performs\nsignificantly outperforms the generic architectures of the same parameter\ncapacity, and also gains remarkable margins over several state-of-the-art\nmethods.", "cluster": "3", "citations": [], "references": [3319912, 220124383, 270906298, 224092448, 220343922, 257409887, 215616963, 216792695, 7017915, 264979485], "authors": ["Zhangyang Wang", "Shiyu Chang", "Jiayu Zhou", "Thomas S. Huang"], "title": "Learning A Task-Specific Deep Architecture For Clustering"}