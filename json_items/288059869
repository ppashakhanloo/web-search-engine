{"references": [228095582, 283475299, 260232379, 265908778, 263011979, 272423025, 269692923, 220320677, 281670476, 277959098], "title": "Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks", "abstract": "Effective training of deep neural networks suffers from two main issues. The\nfirst is that the parameter spaces of these models exhibit pathological\ncurvature. Recent methods address this problem by using adaptive\npreconditioning for Stochastic Gradient Descent (SGD). These methods improve\nconvergence by adapting to the local geometry of parameter space. A second\nissue is overfitting, which is typically addressed by early stopping. However,\nrecent work has demonstrated that Bayesian model averaging mitigates this\nproblem. The posterior can be sampled by using Stochastic Gradient Langevin\nDynamics (SGLD). However, the rapidly changing curvature renders default SGLD\nmethods inefficient. Here, we propose combining adaptive preconditioners with\nSGLD. In support of this idea, we give theoretical properties on asymptotic\nconvergence and predictive risk. We also provide empirical results for Logistic\nRegression, Feedforward Neural Nets, and Convolutional Neural Nets,\ndemonstrating that our preconditioned SGLD method gives state-of-the-art\nperformance on these models.", "authors": ["Chunyuan Li", "Changyou Chen", "David Carlson", "Lawrence Carin"], "citations": [], "id": 288059869}