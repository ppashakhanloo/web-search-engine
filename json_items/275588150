{"references": [266271350, 232805135, 256504640, 265507008, 2543633, 220320048, 235409940, 265469282, 257069490, 228092206], "title": "On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes", "abstract": "The variational framework for learning inducing variables Titsias (2009) has\nhad a large impact on the Gaussian process literature. The framework may be\ninterpreted as minimizing a rigorously defined Kullback-Leibler divergence\nbetween the approximate and posterior processes. To our knowledge this\nconnection has thus far gone unremarked in the literature. Many of the\ntechnical requirements for such a result were derived in the pioneering work of\nSeeger (2003,2003b). In this work we give a relatively gentle and largely\nself-contained explanation of the result. The result is important in\nunderstanding the variational inducing framework and could lead to principled\nnovel generalizations.", "authors": ["Alexander G. de G. Matthews", "James Hensman", "Richard E. Turner", "Zoubin Ghahramani"], "citations": [278332447], "id": 275588150}