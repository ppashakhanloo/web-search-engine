{"references": [51951195, 2603941, 221620338, 228671354, 220232778, 221346235, 221618718, 261153548, 221304322, 221023215], "title": "Asynchronous Parallel Stochastic Gradient Descent - A Numeric Core for Scalable Distributed Machine Learning Algorithms", "abstract": "The implementation of a vast majority of machine learning (ML) algorithms\nboils down to solving a numerical optimization problem. In this context,\nStochastic Gradient Descent (SGD) methods have long proven to provide good\nresults, both in terms of convergence and accuracy. Recently, several\nparallelization approaches have been proposed in order to scale SGD to solve\nvery large ML problems. At their core, most of these approaches are following a\nmap-reduce scheme. This paper presents a novel parallel updating algorithm for\nSGD, which utilizes the asynchronous single-sided communication paradigm.\nCompared to existing methods, ASGD provides faster (or at least equal)\nconvergence, close to linear scaling and stable accuracy.", "authors": ["Janis Keuper", "Franz-Josef Pfreundt"], "citations": [282604207], "id": 277023572}