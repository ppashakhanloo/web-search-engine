{"references": [11500673, 259390620, 257618460, 267759656, 268079368, 41781406, 2834582, 227701452, 260089482, 41781429], "title": "MCMC for Variationally Sparse Gaussian Processes", "abstract": "  Gaussian process (GP) models form a core part of probabilistic machine\nlearning. Considerable research effort has been made into attacking three\nissues with GP models: how to compute efficiently when the number of data is\nlarge; how to approximate the posterior when the likelihood is not Gaussian and\nhow to estimate covariance function parameter posteriors. This paper\nsimultaneously addresses these, using a variational approximation to the\nposterior which is sparse in support of the function but otherwise free-form.\nThe result is a Hybrid Monte-Carlo sampling scheme which allows for a\nnon-Gaussian approximation over the function values and covariance parameters\nsimultaneously, with efficient computations based on inducing-point sparse GPs.\nCode to replicate each experiment in this paper will be available shortly. ", "authors": ["James Hensman", "Alexander G. de G. Matthews", "Maurizio Filippone", "Zoubin Ghahramani"], "citations": [280773011], "id": 278332447}