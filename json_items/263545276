{"references": [51941515, 247818713, 221497549, 221497328, 237415931, 51915335, 262489120, 224471879, 221497234, 220320322], "title": "Unimodal Bandits without Smoothness", "abstract": "We consider stochastic bandit problems with a continuum set of arms and where\nthe expected reward is a continuous and unimodal function of the arm. No\nfurther assumption is made regarding the smoothness and the structure of the\nexpected reward function. We propose Stochastic Pentachotomy (SP), an algorithm\nfor which we derive finite-time regret upper bounds. In particular, we show\nthat, for any expected reward function $\\mu$ that behaves as\n$\\mu(x)=\\mu(x^\\star)-C|x-x^\\star|^\\xi$ locally around its maximizer $x^\\star$\nfor some $\\xi, C>0$, the SP algorithm is order-optimal, i.e., its regret scales\nas $O(\\sqrt{T\\log(T)})$ when the time horizon $T$ grows large. This regret\nscaling is achieved without the knowledge of $\\xi$ and $C$. Our algorithm is\nbased on asymptotically optimal sequential statistical tests used to\nsuccessively prune an interval that contains the best arm with high\nprobability. To our knowledge, the SP algorithm constitutes the first\nsequential arm selection rule that achieves a regret scaling as $O(\\sqrt{T})$\nup to a logarithmic factor for non-smooth expected reward functions, as well as\nfor smooth functions with unknown smoothness.", "authors": ["Richard Combes", "Alexandre Proutiere"], "citations": [283468690], "id": 263545276}