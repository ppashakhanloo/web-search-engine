{"references": [220452657, 222453050, 234793335, 221497426, 221497447, 232568297, 220344008, 247645578, 220343611, 3498822], "title": "Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications", "abstract": "We describe a generalization of the PAC learning model that is based on statistical decision theory. In this model the learner receives randomly drawn examples, each example consisting of an instance x ∈ X and an outcome y ∈ Y, and tries to find a decision rule h: X → A, where h ∈ , that specifies the appropriate action a ∈ A to take for each instance x in order to minimize the expectation of a loss l(y, a). Here X, Y, and A are arbitrary sets, l is a real-valued function, and examples are generated according to an arbitrary joint distribution on X × Y. Special cases include the problem of learning a function from X into Y, the problem of learning the conditional probability distribution on Y given X (regression), and the problem of learning a distribution on X (density estimation). We give theorems on the uniform convergence of empirical loss estimates to true expected loss rates for certain decision rule spaces , and show how this implies learnability with bounded sample size, disregarding computational complexity. As an application, we give distribution-independent upper bounds on the sample size needed for learning with feedforward neural networks. Our theorems use a generalized notion of VC dimension that applies to classes of real-valued functions, adapted from Vapnik and Pollard's work, and a notion of capacity and metric dimension for classes of functions that map into a bounded metric space.", "authors": ["David Haussler"], "citations": [270515202, 233871440, 263790633, 221606003, 47438436, 45921993, 45913941, 45902397, 220618212, 45872876], "id": 222190436}