{"references": [257016708, 45857961, 234168781, 234140352, 228438518, 269280638, 269280638], "title": "Asymptotic analysis of covariance parameter estimation for Gaussian processes in the misspecified case", "abstract": "In parametric estimation of covariance function of Gaussian processes, it is\noften the case that the true covariance function does not belong to the\nparametric set used for estimation. This situation is called the misspecified\ncase. In this case, it has been observed that, for irregular spatial sampling\nof observation points, Cross Validation can yield smaller prediction errors\nthan Maximum Likelihood. Motivated by this comparison, we provide a general\nasymptotic analysis of the misspecified case, for independent observation\npoints with uniform distribution. We prove that the Maximum Likelihood\nestimator asymptotically minimizes a Kullback-Leibler divergence, within the\nmisspecified parametric set, while Cross Validation asymptotically minimizes\nthe integrated square prediction error. In a Monte Carlo simulation, we show\nthat the covariance parameters estimated by Maximum Likelihood and Cross\nValidation, and the corresponding Kullback-Leibler divergences and integrated\nsquare prediction errors, can be strongly contrasting. On a more technical\nlevel, we provide new increasing-domain asymptotic results for the situation\nwhere the eigenvalues of the covariance matrices involved are not upper\nbounded.", "authors": ["Fran√ßois Bachoc"], "citations": [277895584], "id": 269280638}