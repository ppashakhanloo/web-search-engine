{"references": [221620291, 221345346, 4165600, 221619731, 220924212, 32896369, 4326514, 5948829, 220319974, 261683938], "title": "Asynchronous Distributed Learning of Topic Models.", "abstract": "Distributed learning is a problem of fundamental interest in machine learning and cognitive science. In this paper, we present asynchronous distributed learning al- gorithms for two well-known unsupervised learning frameworks: Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Processes (HDP). In the proposed approach, the data are distributed across P processors, and processors indepen- dently perform Gibbs sampling on their local data and communicate their infor- mation in a local asynchronous manner with other processors. We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard L DA and HDP samplers, but with significant improvements in computation time and me mory. We show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processor s. As a stepping stone in the development of asynchronous HDP, a parallel HDP sampler is also introduced.", "authors": ["Arthur U. Asuncion", "Padhraic Smyth", "Max Welling"], "citations": [280221142, 269722321, 268155582, 224907478, 224216843, 254008792, 221520030, 220479741, 51917690, 220320831], "id": 221619032}