{"references": [2433873, 258816388, 276461470, 262604041, 220133126, 2179633, 272194743, 220646955, 276461438, 277959140], "title": "Understanding symmetries in deep networks", "abstract": "Recent works have highlighted scale invariance or symmetry present in the\nweight space of a typical deep network and the adverse effect it has on the\nEuclidean gradient based stochastic gradient descent optimization. In this\nwork, we show that a commonly used deep network, which uses convolution, batch\nnormalization, reLU, max-pooling, and sub-sampling pipeline, possess more\ncomplex forms of symmetry arising from scaling-based reparameterization of the\nnetwork weights. We propose to tackle the issue of the weight space symmetry by\nconstraining the filters to lie on the unit-norm manifold. Consequently,\ntraining the network boils down to using stochastic gradient descent updates on\nthe unit-norm manifold. Our empirical evidence based on the MNIST dataset shows\nthat the proposed updates improve the test performance beyond what is achieved\nwith batch normalization and without sacrificing the computational efficiency\nof the weight updates.", "authors": ["Vijay Badrinarayanan", "Bamdev Mishra", "Roberto Cipolla"], "citations": [283531387], "id": 283532009}