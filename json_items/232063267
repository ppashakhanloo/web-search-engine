{"references": [2559448, 221653277, 221618332, 221653450, 221299559, 224872111, 3457969, 221620432, 220698851, 221618817], "title": "Online Belief Propagation for Topic Modeling", "abstract": "The batch latent Dirichlet allocation (LDA) algorithms play important roles\nin probabilistic topic modeling, but they are not suitable for processing big\ndata streams due to high time and space compleixty. Online LDA algorithms can\nnot only extract topics from big data streams with constant memory\nrequirements, but also detect topic shifts as the data stream flows. In this\npaper, we present a novel and easy-to-implement online belief propagation (OBP)\nalgorithm that infers the topic distribution from the previously unseen\ndocuments incrementally within the stochastic approximation framework. We\ndiscuss intrinsic relations between OBP and online expectation-maximization\n(OEM) algorithms, and show that OBP can converge to the local stationary point\nof the LDA's likelihood function. Extensive empirical studies confirm that OBP\nsignificantly reduces training time and memory usage while achieves a much\nlower predictive perplexity when compared with current state-of-the-art online\nLDA algorithms. Due to its ease of use, fast speed and low memory usage, OBP is\na strong candidate for becoming the standard online LDA algorithm.", "authors": ["Jia Zeng", "Zhi-Qiang Liu", "Xiao-Qin Cao"], "citations": [258566952, 271738230, 262452147], "id": 232063267}