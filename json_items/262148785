{"references": [221995544, 24365499, 239006410, 2456088, 220320160, 2580330, 43247171, 2851008, 220694713, 260354210], "title": "Efficient Probabilistic Classification Vector Machine With Incremental Basis Function Selection", "abstract": "Probabilistic classification vector machine (PCVM) is a sparse learning approach aiming to address the stability problems of relevance vector machine for classification problems. Because PCVM is based on the expectation maximization algorithm, it suffers from sensitivity to initialization, convergence to local minima, and the limitation of Bayesian estimation making only point estimates. Another disadvantage is that PCVM was not efficient for large data sets. To address these problems, this paper proposes an efficient PCVM (EPCVM) by sequentially adding or deleting basis functions according to the marginal likelihood maximization for efficient training. Because of the truncated prior used in EPCVM, two approximation techniques, i.e., Laplace approximation and expectation propagation (EP), have been used to implement EPCVM to obtain full Bayesian solutions. We have verified Laplace approximation and EP with a hybrid Monte Carlo approach. The generalization performance and computational effectiveness of EPCVM are extensively evaluated. Theoretical discussions using Rademacher complexity reveal the relationship between the sparsity and the generalization bound of EPCVM.", "authors": ["Huanhuan Chen", "Peter Tino", "Xin Yao"], "citations": [291183424, 270705722, 279313308, 281338871], "id": 262148785}