{"references": [268988199, 220733897, 228095594, 262877889, 269997903, 255173850, 13853244, 252154212, 269935079, 259400035], "title": "A Recurrent Latent Variable Model for Sequential Data", "abstract": "In this paper, we explore the inclusion of random variables into the dynamic\nlatent state of a recurrent neural network (RNN) by combining elements of the\nvariational autoencoder. We argue that through the use of high-level latent\nrandom variables, our variational RNN (VRNN) is able to learn to model the kind\nof variability observed in highly-structured sequential data (such as speech).\nWe empirically evaluate the proposed model against related sequential models on\nfive sequence datasets, four of speech and one of handwriting. Our results show\nthe importance of the role random variables can play in the RNN dynamic latent\nstate.", "authors": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron Courville", "Yoshua Bengio"], "citations": [284788010, 284579582, 284219020, 282181295], "id": 277959228}