{"references": [240191894, 23162382, 12291808, 242530111, 220698959, 41166614, 221345785, 221345336, 257618460, 2320571], "title": "Kernelized Bayesian Matrix Factorization", "abstract": "We extend kernelized matrix factorization with a fully Bayesian treatment and\nwith an ability to work with multiple side information sources expressed as\ndifferent kernels. Kernel functions have been introduced to matrix\nfactorization to integrate side information about the rows and columns (e.g.,\nobjects and users in recommender systems), which is necessary for making\nout-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite\ngraph inference, where the output matrix is binary, but extensions to more\ngeneral matrices are straightforward. We extend the state of the art in two key\naspects: (i) A fully conjugate probabilistic formulation of the kernelized\nmatrix factorization problem enables an efficient variational approximation,\nwhereas fully Bayesian treatments are not computationally feasible in the\nearlier approaches. (ii) Multiple side information sources are included,\ntreated as different kernels in multiple kernel learning that additionally\nreveals which side information sources are informative. Our method outperforms\nalternatives in predicting drug-protein interactions on two data sets. We then\nshow that our framework can also be used for solving multilabel learning\nproblems by considering samples and labels as the two domains where matrix\nfactorization operates on. Our algorithm obtains the lowest Hamming loss values\non 10 out of 14 multilabel classification data sets compared to five\nstate-of-the-art multilabel learning algorithms.", "authors": ["Mehmet GÃ¶nen", "Suleiman A. Khan", "Samuel Kaski"], "citations": [269721692, 264091340, 269933204, 279992062, 274729851], "id": 232906044}