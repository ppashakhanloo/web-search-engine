{"references": [220320048, 48203914], "title": "Deep Gaussian Processes", "abstract": "In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a\ndeep belief network based on Gaussian process mappings. The data is modeled as\nthe output of a multivariate GP. The inputs to that Gaussian process are then\ngoverned by another GP. A single layer model is equivalent to a standard GP or\nthe GP latent variable model (GPLVM). We perform inference in the model by\napproximate variational marginalization. This results in a strict lower bound\non the marginal likelihood of the model which we use for model selection\n(number of layers and nodes per layer). Deep belief networks are typically\napplied to relatively large data sets using stochastic gradient descent for\noptimization. Our fully Bayesian treatment allows for the application of deep\nmodels even when data is scarce. Model selection by our variational bound shows\nthat a five layer hierarchy is justified even when modelling a digit data set\ncontaining only 150 examples.", "authors": ["Andreas C. Damianou", "Neil D. Lawrence"], "citations": [288713675, 287249271, 284476391, 283619654, 282359718, 283296491, 268079368, 267157313, 235638891, 280894863], "id": 232805135}