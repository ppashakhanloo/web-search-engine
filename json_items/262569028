{"references": [2433873, 220320763, 224881739, 222457504, 37404566, 1903443, 266525095, 221618332, 222502120, 23527871], "title": "Stochastic variational inference for large-scale discrete choice models using adaptive batch sizes", "abstract": "Discrete choice models describe the choices made by decision makers among\nalternatives and play an important role in transportation planning, marketing\nresearch and other applications. The mixed multinomial logit (MMNL) model is a\npopular discrete choice model that captures heterogeneity in the preferences of\ndecision makers through random coefficients. While Markov chain Monte Carlo\nmethods provide the Bayesian analogue to classical procedures for estimating\nMMNL models, computations can be prohibitively expensive for large datasets.\nApproximate inference can be obtained using variational methods at a lower\ncomputational cost with competitive accuracy. In this paper, we develop\nvariational methods for estimating MMNL models that allow random coefficients\nto be correlated in the posterior and can be extended to large-scale datasets.\nWe explore three alternatives: (1) Laplace variational inference, (2)\nnonconjugate variational message passing and (3) stochastic linear regression,\nand compare their performances using real and simulated data. To accelerate\nconvergence for large datasets, we develop stochastic variational inference for\nMMNL models using each of the above three alternatives. Stochastic variational\ninference allows data to be processed in minibatches by optimizing global\nvariational parameters using stochastic gradient approximation. A novel\nstrategy for increasing minibatch sizes adaptively within stochastic\nvariational inference is proposed.", "authors": ["Linda S. L. Tan"], "citations": [], "id": 262569028}