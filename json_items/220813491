{"references": [221620168, 14562158, 264955448, 221618124, 8328258, 1759771, 2952930, 3032582, 228685421, 228855445], "title": "Exponential Family Sparse Coding with Application to Self-taught Learning.", "abstract": "Abstract Sparse coding is an unsupervised,learning algo- rithm for finding concise, slightly higher-level rep- resentations of inputs, and has been successfully applied to self-taught learning, where the goal is to use unlabeled data to help on a supervised learning task, even if the unlabeled data cannot be associ- ated with the labels of the supervised task [Raina et al., 2007]. However, sparse coding uses a Gaussian noise model and a quadratic loss function, and thus performs poorly if applied to binary valued, integer valued, or other non-Gaussian data, such as text. Drawing on ideas from generalized linear models (GLMs), we present a generalization of sparse cod- ing to learning with data drawn from any exponen- tial family distribution (such as Bernoulli, Poisson, etc). This gives a method,that we argue is much better suited to model other data types than Gaus- sian. We present an algorithm for solving the L1- regularized optimization problem,defined by this model, and show that it is especially efficient when the optimal solution is sparse. We also show that the new,model,results in significantly improved self-taught learning performance,when,applied to text classification and to a robotic perception task.", "authors": ["Honglak Lee", "Rajat Raina", "Alex Teichman", "Andrew Y. Ng"], "citations": [282495706, 269630292, 286594619, 236661589, 224243764, 221653243, 221023264, 266069678, 220319862, 224237584], "id": 220813491}