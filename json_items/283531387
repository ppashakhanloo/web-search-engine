{"references": [220693013, 2433873, 277334270, 51957558, 256091783, 220646955, 258816388, 283532009, 279632975, 220133126], "title": "Symmetry-invariant optimization in deep networks", "abstract": "Recent works have highlighted scale invariance or symmetry that is present in\nthe weight space of a typical deep network and the adverse effect that it has\non the Euclidean gradient based stochastic gradient descent optimization. In\nthis work, we show that these and other commonly used deep networks, such as\nthose which use a max-pooling and sub-sampling layer, possess more complex\nforms of symmetry arising from scaling based reparameterization of the network\nweights. We then propose two symmetry-invariant gradient based weight updates\nfor stochastic gradient descent based learning. Our empirical evidence based on\nthe MNIST dataset shows that these updates improve the test performance without\nsacrificing the computational efficiency of the weight updates. We also show\nthe results of training with one of the proposed weight updates on an image\nsegmentation problem.", "authors": ["Vijay Badrinarayanan", "Bamdev Mishra", "Roberto Cipolla"], "citations": [], "id": 283531387}