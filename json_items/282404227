{"references": [215722307, 268688502, 265426449, 224217804, 3309413, 3003665, 3007175, 265426449, 3003941, 3320333], "title": "Derivation of the PHD and CPHD Filters Based on Direct Kullback–Leibler Divergence Minimization", "abstract": "In this paper, we provide novel derivations of the probability hypothesis density (PHD) and cardinalised PHD (CPHD) filters without using probability generating functionals or functional derivatives. We show that both the PHD and CPHD filters fit in the context of assumed density filtering and implicitly perform Kullback–Leibler divergence (KLD) minimizations after the prediction and update steps. We perform the KLD minimizations directly on the multitarget prediction and posterior densities.", "authors": ["Angel F. Garcia-Fernandez", "Ba-Ngu Vo"], "citations": [], "id": 282404227}