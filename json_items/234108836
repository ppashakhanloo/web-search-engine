{"references": [3080334, 215721589, 248034706, 236136582, 40498233, 40498341, 235356636, 235357338, 2537463, 2837633], "title": "Expectation Propagation for Approximate Bayesian Inference", "abstract": "This paper presents a new deterministic approximation technique in Bayesian\nnetworks. This method, \"Expectation Propagation\", unifies two previous\ntechniques: assumed-density filtering, an extension of the Kalman filter, and\nloopy belief propagation, an extension of belief propagation in Bayesian\nnetworks. All three algorithms try to recover an approximate distribution which\nis close in KL divergence to the true distribution. Loopy belief propagation,\nbecause it propagates exact belief states, is useful for a limited class of\nbelief networks, such as those which are purely discrete. Expectation\nPropagation approximates the belief states by only retaining certain\nexpectations, such as mean and variance, and iterates until these expectations\nare consistent throughout the network. This makes it applicable to hybrid\nnetworks with discrete and continuous nodes. Expectation Propagation also\nextends belief propagation in the opposite direction - it can propagate richer\nbelief states that incorporate correlations between nodes. Experiments with\nGaussian mixture models show Expectation Propagation to be convincingly better\nthan methods with similar computational cost: Laplace's method, variational\nBayes, and Monte Carlo. Expectation Propagation also provides an efficient\nalgorithm for training Bayes point machine classifiers.", "authors": ["Thomas P. Minka"], "citations": [285648939, 284476299, 283762222, 282404227, 283117826, 281895982, 281768567, 281144973, 279458734, 279310346], "id": 234108836}